# Financial Query Agent - Interview Talking Points

## Overview

**Project**: A stateful financial analysis agent built with **LangGraph + LangChain + Claude** that orchestrates a multi-step workflow to analyze stocks, calculate technical indicators, reason over market data, and validate recommendations with guardrails.

**Why this matters for Wells Fargo**: Demonstrates production-grade agentic system design that aligns with how trading/sales workflows need stateful orchestration, tool integration, and risk mitigation.

---

## Key Technical Points

### 1. **Agentic Orchestration (Stateful Workflows)**

**The Ask**: "Tell us about how you'd design a multi-step agentic system."

**Your Answer**:
> "I built a LangGraph StateGraph with five orchestrated nodes: **parse_query** â†’ **fetch_data** â†’ **analyze** â†’ **reason** â†’ **validate**. Each node is independent but shares a mutable AgentState object that flows through the graph. This allows me to:
> 
> 1. **Track context across steps**: The agent sees conversation history, prior analysis results, and tool outputs
> 2. **Make decisions dynamically**: After fetching data, the agent analyzes it. After reasoning, it validates the output
> 3. **Handle errors gracefully**: Each node can fail independently; the state tracks what succeeded/failed
> 
> This is critical for trading workflows where you might need to: fetch real-time data â†’ compute Greeks on options â†’ compare with portfolio â†’ generate alerts. The agent needs to remember what it's already done and build on it."

**What They Want to Hear**:
- You understand state management (not just LLM calls in isolation)
- You can design graphs/workflows (DAG thinking)
- You relate it to real trading workflows

---

### 2. **Tool Development & Integration**

**The Ask**: "How do you design tools for agents?"

**Your Answer**:
> "I created three composable tools:
> 
> 1. **StockDataTool**: Wraps yfinance API. Includes caching (stock data doesn't change minute-to-minute), error handling (fallback to mock data), and structured output. The agent calls it with symbol + period, gets back price/volume/history.
> 
> 2. **IndicatorsTool**: Pure calculation engine. Takes price history â†’ outputs RSI, volatility, momentum, MAs. This is **stateless**â€”same input always gives same output. Critical for reliability in prod.
> 
> 3. **PortfolioTool**: Analyzes allocations, suggests rebalancing. Could integrate with a portfolio management system.
> 
> Each tool is:
> - **Independent**: Can be tested & used outside the agent
> - **Typed**: Pydantic models for inputs/outputs. Catches bugs early
> - **Composable**: The agent chains them: fetch â†’ analyze â†’ compare
> - **Observable**: Tool calls are logged (transparency for auditing)
> 
> For Wells Fargo, this pattern scales: MarketDataTool â†’ RiskCalculationTool â†’ ExecutionEngine â†’ ComplianceValidator."

**What They Want to Hear**:
- Tools aren't just "call an API from the LLM"â€”they're software components
- Reliability & composability matter
- You think about observability & auditing

---

### 3. **State Management & Memory**

**The Ask**: "How do you handle multi-turn conversations or evolving state?"

**Your Answer**:
> "I designed the AgentState to track:
> - **Conversation history**: All messages (user/assistant/system). The agent can reflect on what's been said
> - **Analysis results**: Latest stock data, indicators, comparison results
> - **Tool calls**: Complete log of what tools were invoked, their inputs, outputs. Crucial for debugging & auditing
> - **Portfolio context**: Optional holdings so recommendations can be personalized
> 
> Example flow:
> ```
> User: 'Analyze AAPL'
> â†’ State stores query
> â†’ Fetch AAPL â†’ state.latest_data = {...}
> â†’ Analyze â†’ state.latest_indicators = {...}
> â†’ Reason (LLM has context of all prior state)
> â†’ Validate (check reasoning against tool outputs)
> ```
> 
> This is **not** passing data as strings in prompts. The state object *is* the memory. This matters for trading because you need to track: position size, P&L, Greeks, hedge ratiosâ€”all evolving with new market data."

**What They Want to Hear**:
- State â‰  conversation history. It's structured data
- You can track complex financial context (positions, Greeks, etc.)
- Stateful systems scale better than stateless prompts

---

### 4. **Guardrails & Safety (LLM-as-Judge)**

**The Ask**: "How do you prevent hallucinations or overconfidence, especially in regulated domains?"

**Your Answer**:
> "Financial advice is regulated. I built a **GuardrailValidator** that runs LLM-as-judge checks:
> 
> 1. **Overconfidence detection**: Regex checks for 'you must', 'guaranteed', 'will definitely'. Flags as FAIL.
> 2. **Disclaimer check**: Must include 'not financial advice'
> 3. **Confidence score**: Recommendation must include explicit score (0.0-1.0)
> 4. **Reasoning check**: Must cite specific indicators (RSI, volatility, momentum) with values
> 5. **Hallucination detection**: Flags predictive claims ('will rise tomorrow') without supporting logic
> 
> Example:
> - âœ“ GOOD: 'AAPL RSI=72 (overbought). Consider trimming. Confidence: 0.65. Not financial advice.'
> - âœ— BAD: 'Buy AAPL now, guaranteed to go up 20%.'
> 
> My agent passes all 5 checks (1.0 score). For Wells Fargo's compliance, you'd extend this: add NLP checks for latency words, add audit logging, integrate with a policy engine."

**What They Want to Hear**:
- You understand compliance/regulated sectors matter
- LLM-as-judge is a real technique (meta-prompting)
- You can quantify safety (scoring)

---

### 5. **Prompt Engineering & Few-Shot Learning**

**The Ask**: "How do you guide the LLM to reason correctly?"

**Your Answer**:
> "I use three levels of prompting:
> 
> 1. **System prompt**: Sets agent role & constraints
>    - 'You are a financial analyst with X responsibilities'
>    - 'Always cite data sources'
>    - 'Use conditional language, not absolutes'
> 
> 2. **Reasoning prompt**: Gives step-by-step guidance
>    - 'Parse query â†’ Plan tools â†’ Execute â†’ Analyze â†’ Recommend'
>    - Few-shot examples of good analyses
> 
> 3. **Tool use**: Rather than asking LLM to *do* calculations, give it tools
>    - LLM: 'I need volatility'
>    - Agent: Calls IndicatorsTool(AAPL) â†’ returns 18.5%
>    - LLM: 'Good, volatility is moderate, so hedge caution may be warranted'
> 
> **Key insight**: Tool use prevents hallucination. The LLM doesn't invent numbers; it gets them from tools. This is why ReAct patterns workâ€”reasoning + tools + iteration."

**What They Want to Hear**:
- Prompting is engineering, not art
- Few-shot examples and tool use beat raw instruction
- You prevent hallucination by moving computation out of the LLM

---

### 6. **Type Safety & Production Code**

**The Ask**: "How do you ensure code quality?"

**Your Answer**:
> "I use:
> 
> 1. **Pydantic models**: All data has explicit schemas
>    - `Message(role: str, content: str, timestamp: datetime)`
>    - `AnalysisResult(query_symbol: str, rsi: Optional[float], ...)`
>    - Runtime validation catches bugs early
> 
> 2. **Type hints**: Every function is typed
>    - `def fetch_data(state: AgentState) -> AgentState`
>    - Enables IDE autocomplete, mypy checks
> 
> 3. **Logging**: Every step logs what it did
>    - Parse: 'Extracted symbols: AAPL'
>    - Fetch: 'Got data: $180.50'
>    - Validate: 'Guardrail score: 0.95'
>    - This matters for debugging production systems
> 
> 4. **Tests**: Unit tests for tools, integration tests for workflows
>    - Indicator calculation is deterministic â†’ test it
>    - State transitions are predictable â†’ test them
> 
> **Why it matters for Wells Fargo**: Trading systems need audit trails. Every decision is logged. Type safety catches bugs before prod."

**What They Want to Hear**:
- Production code â‰  notebook code
- Type safety and logging are non-negotiable
- You think about auditing from day one

---

## Interview Script: "Walk Us Through Your Project"

### Setup
"I built a financial query agent that demonstrates how to architect stateful agentic workflows. It's production-oriented: typed, logged, tested, with guardrails. I'll walk through the flow."

### 1. Architecture (30 seconds)
"The agent is a LangGraph StateGraph with 5 nodes. User asks a question. The agent:
1. Parses it (extract symbols, intent)
2. Fetches data (real or mock)
3. Analyzes (computes indicators)
4. Reasons (LLM synthesizes analysis)
5. Validates (guardrails check for hallucinations)

All state flows through a mutable AgentState object."

### 2. Tool Design (1 minute)
"I have three tools. StockDataTool fetches prices/volume; caches to avoid API thrashing; falls back to mock if real API fails. IndicatorsTool calculates RSI, volatility, momentumâ€”pure math, no LLM. PortfolioTool does allocation analysis.

Each tool is independent, typed, composable. I can test them without the agent."

### 3. State & Memory (1 minute)
"The state tracks conversation history, latest indicators, tool calls. This matters because the agent needs context. When the LLM reasons, it can see: 'I just fetched AAPL at $180. RSI is 72. Momentum is +5%. What should I recommend?' vs. guessing."

### 4. Guardrails (1 minute)
"The agent is only as good as its outputs. I built a GuardrailValidator that checks:
- No overconfident language (flags 'guaranteed', etc.)
- Has disclaimer
- Has confidence score
- Cites data
- No hallucinated predictions

All five checks must pass. This isn't optional in financeâ€”it's compliance."

### 5. Demo (1 minute)
"Let me show the flow. [Run demo] Query comes in, agent parses it, fetches data, calculates indicators, generates a recommendation, validates it. All guardrails pass. The tool log is transparent: here's what it called, what it got back."

### 6. Wrap-Up
"This architecture scales. For Wells Fargo, imagine replacing 'stock data' with market data microservices, 'indicators' with Greeks calculation, 'validate' with compliance/risk checks. The pattern remains: stateful orchestration, composable tools, guardrails, logging."

---

## How to Use This in Your Interview

### Before the Interview
1. **Run the demo**: `python run_demo.py`
2. **Read the code**: Understand each module (state.py, tools.py, agent.py, guardrails.py)
3. **Practice the script**: Say it out loud a few times
4. **Prepare follow-up answers**:
   - "How would you add real-time market data?"
   - "How would you scale to 1000 concurrent agents?"
   - "What about async/parallelism?"
   - "How do you handle disagreement between tools?"

### During the Interview
- **Start with big picture**: Agentic architecture, not implementation details
- **Use the code as visual aid**: "Here's how I did X..." (show code)
- **Relate to their domain**: "For trading, you'd replace tools with..." (market data, risk, execution, compliance)
- **Mention production concerns**: Logging, monitoring, guardrails, auditing
- **Show humility**: "I focused on architecture and safety; I'd partner with your team on performance optimization"

### Questions They Might Ask

**Q: "How would you handle 10,000 queries per second?"**
A: "StateGraph isn't the bottleneck. The limiting factor is tool latency (API calls). I'd:
   1. Async tools (concurrent API calls)
   2. Caching (stock data, indicator results)
   3. Batch processing (query grouping)
   4. Distributed StateGraph (Langsmith offers checkpointing)
   5. But first: measure. Logging tells you where time is spent."

**Q: "What if tools disagree?"**
A: "Good question. For example, RSI says 'overbought' but momentum says 'bullish'. I'd:
   1. Log both signals explicitly
   2. Let the LLM reason over disagreement
   3. Decrease confidence score if signals conflict
   4. For critical decisions, escalate to human"

**Q: "How do you test this in production?"**
A: "Canary deployment: send 1% of real queries to the new agent, compare outputs with baseline. Monitor guardrail scores, user feedback, accuracy metrics. For financial systems, you'd also do A/B testing with human-reviewed predictions."

**Q: "Why LangGraph over custom orchestration?"**
A: "LangGraph handles checkpointing (save state mid-workflow), branching (conditional nodes), retries (built-in), and integrates with LangSmith (monitoring). Custom orchestration is fun, but you reinvent too much. LangGraph is the ecosystem."

---

## Project Statistics (Good to Mention)

- **Lines of code**: ~1,200 (agent.py, tools.py, state.py, guardrails.py, prompts.py)
- **Test coverage**: 8 test classes, 20+ test methods
- **Guardrail score**: 1.0/1.0 (all 5 checks pass)
- **Time to build**: ~8 hours (architecture + implementation + debugging + docs)
- **Modularity**: 5 independent modules (can swap tools, extend guardrails, etc.)

---

## Follow-Up Projects (If Asked "What's Next?")

1. **Project 2: Java Spring Boot Trade Exposure Service**
   - REST API: `/calculateExposure`, `/checkPosition`
   - Shows Java + API design skills
   - Integrates with Python agent via HTTP
   
2. **Project 3: Real-Time Data Integration**
   - Connect to live market data (Alpaca, Interactive Brokers)
   - WebSocket streaming (async)
   - Shows you can handle real trading workflows

3. **Project 4: Portfolio Optimization**
   - Use agent to suggest portfolio rebalancing
   - Integrate with risk management
   - Shows financial modeling knowledge

---

## Key Takeaway for Wells Fargo

> "I built a small but complete agentic system that demonstrates: stateful orchestration, tool integration, state management, safety guardrails, and production practices. It's not a toyâ€”it's a template for how you'd architect trading agents, sales copilots, or compliance systems at scale."

---

## Code References

- **State management**: [state.py](state.py) - AgentState class tracks all context
- **Tools**: [tools.py](tools.py) - Three independent, composable tools
- **Agent orchestration**: [agent.py](agent.py) - LangGraph StateGraph with 5 nodes
- **Guardrails**: [guardrails.py](guardrails.py) - LLM-as-judge validation
- **Prompts**: [prompts.py](prompts.py) - System prompts + few-shot examples
- **Demo**: [run_demo.py](run_demo.py) - No API keys required, shows full workflow

---

Good luck with your interview! ðŸš€
